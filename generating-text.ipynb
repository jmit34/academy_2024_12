{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431a84f4",
   "metadata": {},
   "source": [
    "# Generating some text\n",
    "\n",
    "Generative-AI is the buzz word since November 2022, when Chat-GPT came out. Previously GANs (Generative Adversarial Networks) had their hours of glory some years ago (circa 2016), and provided spectacular results as computing resources grew. (see for example  [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) )\n",
    "\n",
    "More recently Transformers were able to build Large Language Models (LLMs). \n",
    "\n",
    "Let's have a look, starting with extremly naive attempts to buid some text : \n",
    " \n",
    "0. random letters\n",
    "1. random words \n",
    "2. words with frequencies of sequences\n",
    "3. mini transformer (courtesy of Andrej Karptahy)\n",
    "\n",
    "\n",
    "*Jm Torres, August 2023, torrejm@fr.ibm.com*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5847471",
   "metadata": {},
   "source": [
    "# 0. Random letters.\n",
    "\n",
    "Of course, the results are awfull, but is it very easy to do, so let's do it. \n",
    "With 26 caracters, we can generate strings and try to make it look like some text. \n",
    "\n",
    "1. absolutely random chars,\n",
    "2. random chars grouped in \"words\" of random length. \n",
    "3. random chars grouped in \"words\" of random length, but now we use the actual frequencies of letters in a given language. \n",
    "4. then we can use a realistic distribution of words length (measured on actual text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 just a list of random characters\n",
    "from random import randint\n",
    "size = 200 # nombre de caractères \n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] # liste des caractères\n",
    "\n",
    "# on produit une liste de caractères\n",
    "for i in range(size):\n",
    "    print(chars[randint(0,len(chars)-1)], end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08435d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 random chars with ramdomly selectecd blank characters (random words)\n",
    "from random import randint\n",
    "size = 20 # nombre de mots à produire\n",
    "\n",
    "#word_lengths = [1,2,3,4,5,6,7,8,9,10]\n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] # liste des caractères\n",
    "text = \"\"\n",
    "for i in range(size): \n",
    "    l = randint(1,10) # dans cet exemple on se limite à des mots de longueur maxi 10.\n",
    "    for i in range(l):\n",
    "        text += chars[randint(0,len(chars)-1)]\n",
    "    text += \" \"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca0c5f",
   "metadata": {},
   "source": [
    "Next, we can use the frequencies for each letter in a text, they are available here for a set of languages: \n",
    "\n",
    "https://fr.wikipedia.org/wiki/Fr%C3%A9quence_d%27apparition_des_lettres\n",
    "\n",
    "(we could build this from a large text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66077089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3  groupement par mots, et cette fois les caractères sont tirés au sort,\n",
    "#    mais avec une fréquence ressemblant à celle du français.\n",
    "from random import choices\n",
    "size = 200 # nombre de caractères \n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] # liste des caractères\n",
    "#freq = [71,11,32,37,121,11,12,11,66,3,3,50, 26, 64, 50, 25, 6, 61,65,59, 45, 11, 2,4,5,2 ] # fr\n",
    "freq = [82,15,28,42,127,22,20,61,70,2,8,40, 24, 67, 75, 20, 9, 60,63,90,27, 10, 24,2,20,7,] # en\n",
    "\n",
    "# on produit une liste de caractères\n",
    "print(''.join(choices(chars, weights = freq, k = size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c271b8",
   "metadata": {},
   "source": [
    "Then we cut at random to make words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 en coupant en mots\n",
    "from random import choices\n",
    "size = 20 # nombre de mots\n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] # liste des caractères\n",
    "#freq = [71,11,32,37,121,11,12,11,66,3,3,50, 26, 64, 50, 25, 6, 61,65,59, 45, 11, 2,4,5,2 ]\n",
    "freq = [82,15,28,42,127,22,20,61,70,2,8,40, 24, 67, 75, 20, 9, 60,63,90,27, 10, 24,2,20,7,]\n",
    "text = \"\"\n",
    "for i in range(size): \n",
    "    l = randint(1,10) # dans cet exemple on se limite à des mots de longueur maxi 10.\n",
    "    for i in range(l):\n",
    "        text += choices(chars, freq)[0]\n",
    "    text += \" \"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d789b",
   "metadata": {},
   "source": [
    "With a better than random words length distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.a : fabrication de la distribution des longeurs de mots\n",
    "import string\n",
    "\n",
    "long = {}\n",
    "# mots = \"bonjour tout le monde comment ca va\".split()\n",
    "with open('hp.txt','r') as f:\n",
    "    text = f.read()\n",
    "    text = ' '.join(text.split()) # suppression des espaces multiple\n",
    "    text = text.lower() # suppression des majuscules\n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # supprime ponctu.\n",
    "    mots = text.split() # fait une liste des mots\n",
    "    # print(words)\n",
    "\n",
    "for mot in mots:\n",
    "    long[len(mot)] = long.get(len(mot),0) + 1\n",
    "    \n",
    "long_mot = []\n",
    "freq_mot = []\n",
    "\n",
    "print(\"  length quantity\")\n",
    "print(\"-------- --------\")\n",
    "for k,v in long.items():\n",
    "    if k < 25:  #plus long mot français \n",
    "        print(f\"{k:8.0f}  {v:7.0f}\")\n",
    "        long_mot.append(k)\n",
    "        freq_mot.append(v)\n",
    "\n",
    "lf = list(zip(long_mot, freq_mot))\n",
    "ll,ff = [[i for i, j in sorted(lf)],\n",
    "       [j for i, j in sorted(lf)]]\n",
    "\n",
    "print(ll)\n",
    "print(ff)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ll, ff)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aecaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.b\n",
    "from random import choices\n",
    "size = 20 # nombre de mots\n",
    "chars = [c for c in 'abcdefghijklmnopqrstuvwxyz'] # liste des caractères\n",
    "freq = [82,15,28,42,127,22,20,61,70,2,8,40, 24, 67, 75, 20, 9, 60,63,90,27, 10, 24,2,20,7]\n",
    "\n",
    "text = \"\"\n",
    "for i in range(size): \n",
    "    l = choices(ll,ff)[0]\n",
    "    for i in range(l):\n",
    "        text += choices(chars, freq)[0]\n",
    "    text += \" \"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bba1df",
   "metadata": {},
   "source": [
    "Pour l'anglais la fréquence à utiliser serait (de a à z) : \n",
    "\n",
    "`freq = [82,15,28,42,127,22,20,61,70,2,8,40, 24, 67, 75, 20, 9, 60,63,90,27, 10, 24,2,20,7,]`\n",
    "\n",
    "Pour l'allemand : \n",
    "\n",
    "\n",
    "Pour l'espagnol : \n",
    "\n",
    "\n",
    "Pour le français : \n",
    "\n",
    "`freq = [71,11,32,37,121,11,12,11,66,3,3,50, 26, 64, 50, 25, 6, 61,65,59, 45, 11, 2,4,5,2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffbf468",
   "metadata": {},
   "source": [
    "... ça ne ressemble toujours pas vraiment à un texte ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9aa92",
   "metadata": {},
   "source": [
    "# 1. Random words\n",
    "\n",
    "With random letter we get nowhere, not a single valid word, not even able to tell if it is english or other... \n",
    "\n",
    "So let's start using words, random words(5.0) , using their frequency(5.1)... it looks like english text, but absolutely no meaning... as we could expect \n",
    "\n",
    "Randomness is not usefull in this case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fab607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 \n",
    "\n",
    "long = 100\n",
    "# mots = \"bonjour tout le monde comment ca va\".split()\n",
    "with open('hp.txt','r') as f:\n",
    "    text = f.read()\n",
    "    text = ' '.join(text.split()) # suppression des espaces multiple\n",
    "    text = text.lower() # suppression des majuscules\n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # supprime ponctu.\n",
    "    words = text.split() # fait une liste des mots\n",
    "    # print(words)\n",
    "\n",
    "t = \"\"\n",
    "for i in range(long): \n",
    "    t += words[randint(1,len(words))] + \" \"\n",
    "\n",
    "print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1\n",
    "\n",
    "long = 50\n",
    "\n",
    "with open('hp.txt','r') as f:\n",
    "    text = f.read()\n",
    "    text = ' '.join(text.split()) # suppression des espaces multiple\n",
    "    text = text.lower() # suppression des majuscules\n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # supprime ponctu.\n",
    "    words = text.split() # fait une liste des mots\n",
    "    # print(words)\n",
    "\n",
    "    \n",
    "#words = \"bonjour tout le monde comment ca va le monde\".split()\n",
    "# calcul des fréquences des mots dans le texte. \n",
    "fr = {}\n",
    "for word in words:\n",
    "    fr[word] = fr.get(word,0) + 1 \n",
    "\n",
    "#print(fr)    \n",
    "\n",
    "mots = list(fr.keys())\n",
    "freq = list(fr.values())\n",
    "t = \"\"\n",
    "for i in range(long): \n",
    "    t += choices(mots,freq)[0] + \" \"\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56eb03",
   "metadata": {},
   "source": [
    "# 2. Trying better : from an existing text, let's build a graph that counts the frequencies of the following words for a given word. \n",
    "\n",
    "Using grammatical rules to generate texts is a difficult problem (very difficult in french) this is why other strategies have come up, such as this one (bigrams) \n",
    "\n",
    "\n",
    "From Text to Graph (oriented, and weighted) \n",
    "- Vertices: words  \n",
    "- Edges: \n",
    "    - oriented: 'I' links to 'eat' (end not 'eat' to 'I')\n",
    "    - weighted: proportional the frequency of the link (\"need -> money\" is more frequent than \"need -> hippopotamus\")\n",
    "    \n",
    "From this text :  `je vais à la gare, je vais à la maison pour manger des pâtes, il mange des fruits, elle mange des fruits, je chante à la maison, je vais manger des fuits, à la maison je vais dormir`, this graph can be build: \n",
    "\n",
    "![le graphe correspondant](graphe.png)\n",
    "\n",
    "\n",
    "Once build, the graph can be used to generate a list of words connected by an edge and chosen according to the vertice weight.\n",
    "\n",
    "We can unsterstand that the larger the set of texts used to build the graph, the best the quality of the output. \n",
    "\n",
    "Let's try a small one ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce74e90",
   "metadata": {},
   "source": [
    "### a. building the vertex anf graph classes, with their methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31c64fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Vertex:\n",
    "    ''' Classe pour les sommets (vertices) du graphe, qui représentent les mots\n",
    "        Attributs : \n",
    "        - value (str): le mot lui même\n",
    "        - adjacent{} : le dictionnaire des sommets adjacents (reliés, avec un edge)\n",
    "            les éléments du dictionnaire \"adjacent\" sont {vertex_adjacent1:poids1, ... \n",
    "            vertex_adjacentN:poids:N}\n",
    "            Les poids sont entiers et seront incrémentés au fur et à mesure que le même \n",
    "            lien entre ce vertex et le vertex adjacent_i est retrouvé dans le texte\n",
    "        - neighbors[] : la liste des vertices adjacents\n",
    "        - neighbors_value[] : la liste des valeurs (mot) des vertices adjacents\n",
    "        - neighbors_weights[] : la liste des poids pour les edges correspondants\n",
    "        Ces listes sont extraites du dictionnaire d'adjacence, une fois celui-ci \n",
    "        constitué\n",
    "            '''\n",
    "    def __init__(self,value):\n",
    "        self.value = value  # le mot lui même\n",
    "        self.adjacent = {}  # dictionnaire pour les vertex adjacents\n",
    "        self.neighbors = [] # liste des sommets reliés (ce sont des Vertex)\n",
    "        self.neighbors_value = [] # liste des mots des sommmets reliés\n",
    "        self.neighbors_weights = [] # liste des poids des sommets reliés\n",
    "           \n",
    "    def increment_edge(self,vertex):\n",
    "        \"\"\" incrément du poids de liaison entre self et un autre vertex \"\"\"\n",
    "        self.adjacent[vertex] = self.adjacent.get(vertex,0) + 1\n",
    "    \n",
    "    def get_adjacent_nodes(self):\n",
    "        \"\"\"  renvoie les vertex adjacents \"\"\"\n",
    "        return self.adjacent.keys()\n",
    "    \n",
    "    def vertex_probability_map(self):\n",
    "        \"\"\" fabrication des listes neighbors et neighbors_weights\n",
    "            Les listes neighbors[] et neighbors_weights[] sont utilisés pour \n",
    "            tirer au sort le prochain mot dans l'adjacence de self avec le poids\n",
    "            correspondant, le liste neighbors_value est utilisée par\n",
    "            get_vertex_adjacent_data, pour la méthode __str__. de Graph\"\"\"\n",
    "        for (vertex,weight) in self.adjacent.items():\n",
    "            self.neighbors.append(vertex)\n",
    "            self.neighbors_value.append(vertex.value)\n",
    "            self.neighbors_weights.append(weight)\n",
    "            \n",
    "    #def get_vertex_adjacent_data(self):\n",
    "    #    \"\"\" renvoie la liste des adjacents et leurs poids\n",
    "    #        cette fonction n'est utilisée que pour la méthode __str__. \n",
    "    #        dans la classe Graph \"\"\"\n",
    "    #    return self.neighbors_value, self.neighbors_weights\n",
    "        \n",
    "    def next_word(self):\n",
    "        \"\"\" utilise random.choices pour tirer au sort dans la liste des neighbors\n",
    "            avec pour chacun des neighbors la probabilité neighbors_weight\n",
    "            (choices tire au sort dans une liste avec une liste de poids\n",
    "            choices renvoie une liste (de longueur k, nous on prend 1) \n",
    "            c'est pourquoi pour avoir l'élément on demande l'élément [0] \"\"\"\n",
    "        return random.choices(self.neighbors, weights = self.neighbors_weights, k = 1)[0]\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\" permet d'utiliser la fonction print pour un objet de cette classe\"\"\"\n",
    "        return self.value + \" \" +  ' '.join([node.value for node in self.adjacent.keys()])\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        \"\"\" le graphe est un dictionnaire de sommets : {mot:vertex}\"\"\"\n",
    "        self.vertices = {} \n",
    "        \n",
    "    def get_vertex_values(self):\n",
    "        \"\"\"renvoie tous les sommets (vertices) du graphe\n",
    "           c'est un ensemble de ...\n",
    "        \"\"\"\n",
    "        return set(self.vertices.keys())\n",
    "    \n",
    "    def add_vertex(self, value):\n",
    "        self.vertices[value] = Vertex(value)\n",
    "        \n",
    "    def get_vertex(self, value):\n",
    "        if value not in self.vertices : \n",
    "            self.add_vertex(value)\n",
    "        return self.vertices[value]\n",
    "    \n",
    "    def get_next_word(self,current_vertex):\n",
    "        return self.vertices[current_vertex.value].next_word()\n",
    "        \n",
    "    def generate_probability_mappings(self):\n",
    "        ''' pour chaque mot du graphe on parcours son dictionnaire .adjacent \n",
    "            afin de contruire la liste des mots et des poids'''\n",
    "        for vertex in self.vertices.values():\n",
    "            vertex.vertex_probability_map()\n",
    "            \n",
    "    def __str__(self):\n",
    "        \"\"\" ma fonction print pour un objet de cette classe\"\"\"\n",
    "        p = \"*** début graphe ***\"  \n",
    "        for k,v in self.vertices.items():\n",
    "            p+= \"\\n--- mot ->\" + k + \" -- suivant(s) ->\" + str(v.neighbors_value) + \\\n",
    "            \" -- poid(s) ->\" + str(v.neighbors_weights) + \"\\n\"\n",
    "            p+= \"------\"\n",
    "        p += \"***  fin graphe  ***\"\n",
    "        return p\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10079120",
   "metadata": {},
   "source": [
    "### b. building the graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bb72e",
   "metadata": {},
   "source": [
    "1. words extraction\n",
    "2. graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405641e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string # pour supprimer la poncutation - on peut faire à la main avec regex\n",
    "\n",
    "def get_words_from_text(text_path):\n",
    "    \"\"\" chargement du fichier texte et préparation : \n",
    "            suppression des espaces multiples, des majuscules et des ponctuations.\"\"\"\n",
    "    with open(text_path,'r') as f:\n",
    "        text = f.read()\n",
    "        text = ' '.join(text.split()) # suppression des espaces multiple\n",
    "        text = text.lower() # suppression des majuscules\n",
    "        text = text.translate(str.maketrans('','', string.punctuation)) # supprime ponctu.\n",
    "        words = text.split() # fait une liste des mots\n",
    "        #print(words)\n",
    "        return words\n",
    "        \n",
    "def make_graph(words):\n",
    "    \"\"\" construction du graph \"\"\"\n",
    "    g = Graph()\n",
    "    previous_word = None\n",
    "    # pour chaque mot du texte, s'il n'y est pas on l'ajoute,\n",
    "    # on récupère le vertex\n",
    "    for word in words:\n",
    "        word_vertex = g.get_vertex(word)      \n",
    "        # s'il y avait un previous_word, on ajoute un lien si pas déjà\n",
    "        # si le ien existait, on augmente son poids/sa valeur de 1\n",
    "        if previous_word:\n",
    "            previous_word.increment_edge(word_vertex)\n",
    "            \n",
    "        previous_word = word_vertex\n",
    "    \n",
    "        # on a fini de parcourir words, on va générer les probabilités\n",
    "        # c'est à dire : pour chaque mot, on fait la liste des adjacents et la liste \n",
    "        # des poids, pour pouvoir faire le tirage au sort (random.choices)\n",
    "    g.generate_probability_mappings()\n",
    "   \n",
    "    #print(g)\n",
    "    return g\n",
    "\n",
    "def compose(g, words, length=50):\n",
    "    \"\"\" composition du texte, à partir d'un mot au hasard\n",
    "        TODO : on pourrait faire un prompt d'un seul mot en prenant un\n",
    "        mot de départ en paramètre plutôt que de le choisir au hasard \"\"\"\n",
    "    composition = []\n",
    "    word = g.get_vertex(random.choice(words))\n",
    "    \n",
    "    for _ in range(length):\n",
    "        composition.append(word.value)\n",
    "        word = g.get_next_word(word)\n",
    "        \n",
    "    return composition\n",
    "        \n",
    "# -1- get words from text\n",
    "\n",
    "#words = get_words_from_text(\"hp1.txt\")\n",
    "words = get_words_from_text(\"hp.txt\")\n",
    "print(len(words))\n",
    "\n",
    "#print(words,'\\n\\n')\n",
    "\n",
    "# -2- make a graph using those words\n",
    "    \n",
    "g = make_graph(words)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51137605",
   "metadata": {},
   "source": [
    "### c. generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a59e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -3- faire un séquence de N mots parmi words\n",
    "#     avec la structure du graphe G\n",
    "\n",
    "composition = compose(g,words,200)\n",
    "    \n",
    "print(' '.join(composition)) # on transforme la liste en string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964fc7a",
   "metadata": {},
   "source": [
    "Getting better but still very bad. \n",
    "\n",
    "Indeed, we only use word to word frequencies, one can imagine it will be better if we analyze words relations over longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1448af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# par curiosité, voici à quoi ressemble le fameux graphe : \n",
    "# pour chaque mot du texte, on voit un couple de listes ([],[]) \n",
    "# la première liste contient l'ensemble des mots qui suivent le mot en question\n",
    "# la seconde contient les quantités de fois ou chacun des mots de la première liste \n",
    "# a suivi. \n",
    "\n",
    "print(g)\n",
    "\n",
    "# on voit aussi que pour beaucoup (quantifier ?) de mots, on n'a collecté qu'un seul suivant, \n",
    "# le modèle est assez pauvre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227543b",
   "metadata": {},
   "source": [
    "# 3. So let's build our own GPT (Generative Pre-trained Transformer) \n",
    "\n",
    "All of Sheakespeare:  \n",
    "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "\n",
    "Accounting for relationships over many words is knownn as \"attention\".\n",
    "\n",
    "\n",
    "This concept has been coined in this seminal paper in 2017: \"Attention is all you need\" describing the \"Transformer\": https://arxiv.org/abs/1706.03762\n",
    "\n",
    "\n",
    "The followong code uses the PyTorch library, and is from Youtube, Andreij Karpathy : Let's build GPT: from scratch, in code, spelled out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt > \"input.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c270bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e92220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d8663",
   "metadata": {},
   "source": [
    "On va utiliser des tokens de 1 caractère de long, le plus simple possible. (Google utilise le tokenizer SentencePiece (des tokens qui sont des parties de mots, OpenAI avec GTP2 : 50257 différents token (bouts de mots), les séquences encodées sont plus courtes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping char <-> int\n",
    "# tokenizer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # list of int to string\n",
    "\n",
    "print(encode(\"Bonjour tout le monde\"))\n",
    "print(decode(encode(\"Bonjour tout le monde\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41479ffa",
   "metadata": {},
   "source": [
    "on va utiliser une structure  PyTorch pour stocker nos séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# en cas de souci de performance, on pourra restreindre la taille du texte ici \n",
    "# au moment de la définition de data.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd14441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sépare en entrainement et validation datasets:\n",
    "n = int(0.9*len(data)) \n",
    "# 90% en train le reste en val : \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41417a0",
   "metadata": {},
   "source": [
    "On ne va pas mettre l'ensemble du texte dans le transofrmer d'un coup. (Trop difficile à calculer), on y va par chunk (bout), ou bloc.\n",
    "\n",
    "Voyons par exemple le tout premier bloc que l'on peut extraire de notre texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927976a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 \n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dans ce block, il y a 8 exemples : \n",
    "#  dans le contexte de 18, le suivant est 47\n",
    "#  dans le contexte de 18,47, le suivant est 56\n",
    "#  dans le contexte de 18,47,56 le suivant est 57\n",
    "# ... \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size): \n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, then target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bdd1f",
   "metadata": {},
   "source": [
    "Généralisons, en batch de blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # nombre de séquences à traiter \n",
    "block_size = 8 # longueur du contexte\n",
    "\n",
    "# l'array 4 * 8 contient 32 exemple\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\" générate a small batch of data input x, target y\"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # retourne 4 (bath_size) random indexes\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is: {target}\")\n",
    "    \n",
    "    \n",
    "# le tenseur x va rentrer dans le transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3598fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un nn très simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe45571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both B,T tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # batch, time, channel B T C\n",
    "        if targets is None: \n",
    "            loss = None\n",
    "        else: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T,C) # ca c'est pour des questions de format de torch \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "        return logits, loss # score of what's come next \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss) # -ln(1/65)\n",
    "# on n'a pas fait l'entrainement et on utilise que bigram : [:,-1:] ?\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c78a49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e708314",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100):\n",
    "    xb,yb = get_batch('train')\n",
    "    \n",
    "    logits,loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=1500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96454a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2660e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))  #bow : bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] #(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67af358",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow[0] # moyenne par rapport aux lignes ci-dessus (ligne2 = avg(lig0 et lig1) ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ad661",
   "metadata": {},
   "source": [
    "The mathematical trick in self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f14cf4",
   "metadata": {},
   "source": [
    " we want $x[b,t] = mean_{i<=t} x[b,i]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bb827d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros(B,T,C)\n",
    "for b in range(B):\n",
    "    for t in range(T): \n",
    "        xprev = x[b,:t+1] #(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('---')\n",
    "\n",
    "# le résultat donne la somme des valeur des colonnes de b, sur chaque ligne de c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example en utilisant la matrice triangulaire de 1 en bas à gauche pour 1, \n",
    "# ce qui produit à présent une somme de plus en plus de lignes de b \n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('---')\n",
    "\n",
    "# le résultat donne la somme des valeur des colonnes de b, sur chaque ligne de c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example en utilisant la matrice triangulaire de 1 en bas à gauche pour 1, \n",
    "# ce qui produit à présent une somme de plus en plus de lignes de b \n",
    "# maintenant on normalise a pour que chaque ligne ait pour somme 1\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a,1, keepdim = True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('---')\n",
    "\n",
    "# le résultat donne la somme des averages des colonnes de b, sur chaque ligne de c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265feef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on reprend le fil de la construction\n",
    "wei = torch.tril(torch.ones(T,T)) # wei : short for weights\n",
    "wei = wei / wei.sum(1,keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = wei @ x # wei is T by T, x (B by T by C), pytorch va faire BTT BTC\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xbow[0],\"\\n\",xbow2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50802140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# troisième version : use Softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "#wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230a0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# troisième version : use Softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow,xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b821be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4 : self-attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "# on ne veut plus un monde uniforme\n",
    "# on veut des information des tokens précédents\n",
    "# each node will emit 2 vectors : \n",
    "# query : what am I looking for ?  \n",
    "# key : what do I contain ?\n",
    "# l'affinity entre les token en séquence : \n",
    "#  my_q dot product with all the keys des autres tokens dans la séquence \n",
    "#    et donc que le passé\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape\n",
    "# la minute 1h09 fait un bon résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2806718",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea1a7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k.var(), q.var(), wei.var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19856ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15272e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b71564",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b02bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b74025",
   "metadata": {},
   "source": [
    "#### Ca commence à être pas mal, avec 2 minutes de calcul sur mon mac, et 100 000 paramètres. \n",
    "\n",
    "Les LLM sont dans les 100 milliards de paramètres (soit de l'ordre de million de fois plus grands)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef0c5a",
   "metadata": {},
   "source": [
    "## Sources :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca8070",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90fbb132",
   "metadata": {},
   "source": [
    "\n",
    "- bigrammes : 12 Beginner Python Projects - Coding Course. Chaine Youtube FreeCodeCamp.org : https://www.youtube.com/watch?v=8ext9G7xspg\n",
    "- transformer : Let's build GPT: from scratch, in code, spelled out. Chaine Youtube Andreij Karpathy : https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
